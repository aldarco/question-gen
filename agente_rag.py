# -*- coding: utf-8 -*-
"""agente_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PgaZMlayzJw4pc9Ilfnd5xSFdhXhfJcf
"""

#pip install langchain pypdf faiss-cpu openai tiktoken
#! pip install langchain-community langchain_openai
#! pip install chromadb

#!pip install langchain_huggingface

# Hugging Face
# !pip install transformers sentence-transformers

#from google.colab import userdata
#key = userdata.get('rag1')

import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
#from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface.chat_models import ChatHuggingFace
from langchain_huggingface.llms import HuggingFaceEndpoint

#from google.colab import userdata
#key2 = userdata.get('rag2')
key2 = os.getenv("HFAPIKEY")
#os.environ["OPENAI_API_KEY"] = key
import warnings

warnings.filterwarnings("ignore") #, message="`encoder_attention_mask` is deprecated")


def load_context(pdf_path = "./clase3.pdf"):
    global vectorstore
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    # documentos en chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        add_start_index=True,
    )
    texts = text_splitter.split_documents(documents)
    print(f"Divididos {len(documents)} documentos en {len(texts)} chunks.")
    #embeddings = OpenAIEmbeddings()

    embeddings = HuggingFaceEmbeddings( #SentenceTransformerEmbeddings(
        model_name="all-MiniLM-L6-v2"  # Lightweight model (384-dim)
    )
    print("Vectorizing...")
    vectorstore = Chroma.from_documents(
        documents=texts,           
        embedding=embeddings,
        #persist_directory="./chroma_db" 
    )
    #vectorstore = FAISS.from_documents(texts, embeddings)
    print("Vector Store creada y lista.")
    return vectorstore
# Inicializar el LLM
#llm_model = ChatOpenAI(model="gpt-4.1", temperature=0.7)
'''
llm_model = ChatHuggingFace(
    llm=HuggingFaceEndpoint(
        repo_id= "HuggingFaceH4/zephyr-7b-beta",  # Free open model
        task="text-generation",
        max_new_tokens=512,
        temperature=0.7,
        huggingfacehub_api_token=key2
    )
)
'''
#from langchain_community.chat_models import ChatHuggingFace
#from langchain_community.llms import HuggingFaceEndpoint

llm_model = ChatHuggingFace(
    llm=HuggingFaceEndpoint(
        repo_id="deepseek-ai/deepseek-V3",  # or deepseek-coder-6.7b-instruct
        task="text-generation",
        huggingfacehub_api_token=key2
    )
)


problem_generation_template = """
Eres un experto en la materia de {materia} y tienes la tarea de crear un problema de examen.
El problema debe requerir razonamiento y comprensión de los conceptos.
Utiliza el siguiente texto de clase para asegurar la relevancia y precisión del problema en funcion del contenido proporcionado en clase:
---
{context}
---
Crea {cantidad} problemas de examen claros y conciso sobre el {tema} brindado en el contexto anterior que sea adecuado para estudiantes de {nivel_educativo}.
El problema debe tener una pregunta bien definida. Estas preguntas pueden ser teóricas (conceptuales) o aplicativas, determina aleatoriamente el tipo según la cantidad solicitada de problemas.
Las ecuacione sy la notacion matematica debe estar en formato latex.
"""

problem_prompt = PromptTemplate(
    template=problem_generation_template,
    input_variables=["materia", "cantidad", "tema", "nivel_educativo", "context"]
)

problem_generation_chain = LLMChain(llm=llm_model, prompt=problem_prompt)

def generar_problema(materia, tema,cantidad, nivel_educativo):
    # fitrado por relevancia
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # Recupera los 5 chunks más relevantes
    #docs = retriever.get_relevant_documents(tema)
    docs = retriever.invoke(tema)
    context = "\n".join([doc.page_content for doc in docs])

    if not context:
        print(f"Advertencia: No se encontró contexto relevante para '{tema}'. El problema puede ser más genérico.")

    problema = problem_generation_chain.invoke({
        "materia":materia,
        "tema":tema,
        "cantidad":cantidad,
        "nivel_educativo":nivel_educativo,
        "context":context
    }
    )
    return problema





if __name__ == "__main__":
    from markdown_pdf import MarkdownPdf, Section
    import nbformat as nbf

    nb = nbf.v4.new_notebook()
    print("\n-------------------------")
    print("GENERADOR DE PROBLEMAS")
    print("-------------------------")
    
    materia_ejemplo = input("Curso: ") #"Métodos numéricos"
    tema_ejemplo =  input("Tema: ") #"Solución de ecuaciones no lineales"
    pdf_path = input("Texto Referencia: ")
    nivel_ejemplo = input("Nivel: ") #"universitario"
    cantidad = int(input("Cantidad: "))
    vectorstore = load_context(pdf_path)
    print(f"\nGenerando {cantidad} problemas de {materia_ejemplo} sobre {tema_ejemplo} para nivel {nivel_ejemplo}...")
    problema_generado = generar_problema(materia_ejemplo, tema_ejemplo, cantidad, nivel_ejemplo)
    print("\n------\n Problemas Generados ")
    print(problema_generado["text"])

    filename = f'gen_{tema_ejemplo}_ref-{pdf_path.split(".")[0]}.pdf'
    with open("lastoutput.txt","w", encoding="utf-8") as file:
        file.write(problema_generado["text"])
        #print("> saved MarkDown as lastoutput")
    #pdf = MarkdownPdf()
    #pdf.meta["title"] = "Generated Problems"
    #pdf.add_section(Section(problema_generado["text"], toc=False))
    #pdf.save(filename)
    nb['cells'] = [nbf.v4.new_markdown_cell(problema_generado["text"])]
    with open("problemas.ipynb", "w") as f:
        nbf.write(nb, f)
        print(f"> saved in file as {filename}")
