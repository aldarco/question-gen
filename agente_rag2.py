# -*- coding: utf-8 -*-
"""agente_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PgaZMlayzJw4pc9Ilfnd5xSFdhXhfJcf
"""

#pip install langchain pypdf faiss-cpu openai tiktoken
#! pip install langchain-community langchain_openai
#! pip install chromadb

#!pip install langchain_huggingface

# Hugging Face
# !pip install transformers sentence-transformers

#from google.colab import userdata
#key = userdata.get('rag1')

import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings

from langchain_huggingface.chat_models import ChatHuggingFace
from langchain_huggingface.llms import HuggingFaceEndpoint

#from google.colab import userdata
#key2 = userdata.get('rag2')
key2 = os.getenv("HFAPIKEY")
#os.environ["OPENAI_API_KEY"] = key

# fuenets

pdf_path = "./clase3.pdf"
loader = PyPDFLoader(pdf_path)
documents = loader.load()

# documentos en chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    add_start_index=True,
)
texts = text_splitter.split_documents(documents)
print(f"Divididos {len(documents)} documentos en {len(texts)} chunks.")
#embeddings = OpenAIEmbeddings()

embeddings = SentenceTransformerEmbeddings(
    model_name="all-MiniLM-L6-v2"  # Lightweight model (384-dim)
)
print("Vectorizing...")
vectorstore = Chroma.from_documents(
    documents=texts,           # Document chunks
    embedding=embeddings,
    #persist_directory="./chroma_db"  # save to disk
)

#
#vectorstore = FAISS.from_documents(texts, embeddings)
print("Vector Store creada y lista.")

# Inicializar el LLM
#llm_model = ChatOpenAI(model="gpt-4.1", temperature=0.7)
'''
llm_model = ChatHuggingFace(
    llm=HuggingFaceEndpoint(
        repo_id= "HuggingFaceH4/zephyr-7b-beta",  # Free open model
        task="text-generation",
        max_new_tokens=512,
        temperature=0.7,
        huggingfacehub_api_token=key2
    )
)
'''
#from langchain_community.chat_models import ChatHuggingFace
#from langchain_community.llms import HuggingFaceEndpoint

llm_model = ChatHuggingFace(
    llm=HuggingFaceEndpoint(
        repo_id="deepseek-ai/deepseek-V3",  # or deepseek-coder-6.7b-instruct
        task="text-generation",
        huggingfacehub_api_token=key2
    )
)

# prompt para generar problemas
problem_generation_template = """
Eres un experto en la materia de {materia} y tienes la tarea de crear un problema de examen.
El problema debe requerir razonamiento y comprensión de los conceptos.
Utiliza el siguiente texto de clase para asegurar la relevancia y precisión del problema en funcion del contenido proporcionado en clase:
---
{context}
---
Crea {cantidad} problemas de examen claros y conciso sobre el {tema} brindado en el contexto anterior que sea adecuado para estudiantes de {nivel_educativo}.
El problema debe tener una pregunta bien definida. Estas preguntas pueden ser teóricas (conceptuales) o aplicativas, determina aleatoriamente el tipo según la cantidad solicitada de problemas.
"""

problem_prompt = PromptTemplate(
    template=problem_generation_template,
    input_variables=["materia", "cantidad", "tema", "nivel_educativo", "context"]
)

problem_generation_chain = LLMChain(llm=llm_model, prompt=problem_prompt)

def generar_problema(materia, tema,cantidad, nivel_educativo):
    # fitrado por relevancia
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # Recupera los 5 chunks más relevantes
    docs = retriever.get_relevant_documents(tema)
    context = "\n".join([doc.page_content for doc in docs])

    if not context:
        print(f"Advertencia: No se encontró contexto relevante para '{tema}'. El problema puede ser más genérico.")

    problema = problem_generation_chain.run(
        materia=materia,
        tema=tema,
        cantidad=cantidad,
        nivel_educativo=nivel_educativo,
        context=context
    )
    return problema




"""## Prueba"""

materia_ejemplo = "Métodos numéricos"
tema_ejemplo = "olución de ecuaciones no lineales"
nivel_ejemplo = "universitario"
cantidad = 2

#print(f"\nGenerando {cantidad} problemas de {materia_ejemplo} sobre {tema_ejemplo} para nivel {nivel_ejemplo}...")
#problema_generado = generar_problema(materia_ejemplo, tema_ejemplo, cantidad, nivel_ejemplo)
#print("\n Problemas Generados ")
#print(problema_generado)


import streamlit as st
# ... (todo tu código de LangChain y el agente RAG va aquí) ...

st.title("Agente RAG para Exámenes")

materia = st.text_input("Materia")
tema = st.text_input("Tema")
cantidad = st.text_input("Cantidad")
nivel_educativo = st.selectbox("Nivel Educativo", ["primaria", "secundaria", "universitario"])

if st.button("Generar Problema"):
    if materia and tema and nivel_educativo:
        problema_generado = generar_problema(materia, tema, cantidad, nivel_educativo)
        st.text_area("Problema Generado", problema_generado, height=300)
        st.session_state['problema'] = problema_generado # Guardar en el estado de la sesión
        st.session_state['tema_problema'] = tema # Guardar el tema del problema
    else:
        st.warning("Por favor, rellena todos los campos para generar el problema.")

if 'problema' in st.session_state:
    st.subheader("Evaluar Respuesta")
    respuesta_estudiante = st.text_area("Tu Respuesta Aquí", height=200)

    if st.button("Evaluar Respuesta"):
        if respuesta_estudiante:
            evaluacion_y_feedback = evaluar_respuesta(
                materia,
                st.session_state['problema'],
                respuesta_estudiante,
                st.session_state['tema_problema']
            )
            st.text_area("Evaluación y Feedback", evaluacion_y_feedback, height=400)
        else:
            st.warning("Por favor, ingresa una respuesta para evaluar.")

